<!DOCTYPE html>
<html>

<head>
    <meta charset="utf‐8">
    <title> Automated Composition Visualization </title>
</head>

<body>

    <h2 align="center">
        Automated Composition Visualization Blog Post
    </h2>

    <p align="center">by Sedona Thomas and Carolyn Wang</p>
    <p align="center"><a href="https://github.com/sedona-thomas/Automated-Compotition-Visualization">Github repo</a></p>


    <h3>
        Background
    </h3>
    <p>Writing, visual art, and computer music are all areas of art, which is to say, they are all intensely subjective.
        In this project, we wanted to explore the translations between these artistic mediums by creating a creative
        interpretation of what a "writing style" might look like and sound like, as processed by our mathematical
        understandings and coding decisions, thus putting art forms that are normally separated in conversation with
        each other and giving the user something fun and open-ended to explore.</p>
    <h3>
        Text processing
    </h3>
    <p>Everything the computer produces is based on the initial text input from the user. We then analyze the following
        attributes to determine corresponding characteristics of sound and visuals produced: </p>
    <ul>
        <li><b>Word length:</b> Of course, when someone is speaking, they can choose to speak as slowly or as quickly as
            they
            want to, so there is no fixed length for how long a word might last. To translate that to our computer
            music, we looked at word length as it related to note length. Based on <a
                href="http://norvig.com/mayzner.html">text processing research</a>, the average
            length of a word is 4.79 letters (rounded up to 5 in the code, for convenience; the user will not be
            inputting
            words of length 4.79). In music, we assumed that "the average word" would correspond to "the average note":
            a quarter note played in 4/4 time signature at "moderate tempo," which tops out at a convenient <a
                href="https://www.masterclass.com/articles/music-101-what-is-tempo-how-is-tempo-used-in-music">120
                bpm.</a> From our mathematical calculations, that means the average word (of five letters) would last
            0.5 seconds in sound.</li>
        <li><b>Punctuation:</b> Punctuation is used in speech and in writing often to separate or to create pauses.
            Thus, it makes sense that translating an input with punctuation in it to sound would mean leaving in longer
            gaps between notes where punctuation should be. Based on a subjective ranking of which punctuation marks
            would create the longest pauses (for example, deciding that a dash would create a shorter gap between
            words than a period). </li>
        <li><b>Letter diversity:</b> We created a measurement of letter diversity that we then used to determine
            waveform type. In our measurement of letter diversity, an input is more diverse if the letters that it
            contains are more evenly distributed. This metric only considers the letters that the input actually uses,
            not the entire alphabet; thus, an input like "aaabb" would be considered evenly distributed and "diverse"
            even though it does not use the entire alphabet. We measure letter diversity against the letters in the
            actual input, as opposed to the whole alphabet, so as to not skew the results toward unlikely inputs that
            happen to
            contain every letter of the alphabet. (After all, natural text inputs rarely contain every single letter of
            the alphabet.) The letter diversity calculation ignores punctuation and capitalization. The higher the
            "letter diversity score," the more "diverse" we consider an input. Based on this letter diversity score, we
            change between the waveform types of square, triangle, sine, and sawtooth, with sine and sawtooth being the
            most likely for natural language inputs (those sound very different but also familiar, so the user can
            experience both and occasionally triangle and square, which they might not expect). </li>
        <li><b>Capitalization: </b> We translated words that start with a capital letter into sound by playing them in
            additive synthesis. If capitalization is often used to emphasize or signify importance, it makes sense that
            the subsequent sound also seems emphasized by additional oscillators set at progressively increasing
            octaves. </li>
        <li><b>Word length standard deviation:</b> Using the <a
                href="https://en.wikipedia.org/wiki/Standard_deviation">formula for standard deviation</a>, we
            calculated the standard deviation of word lengths in the given text input to determine the number of
            partials for words that used additive synthesis. This was just a fun metric to implement, to add some
            additional variation to the sound outputs of the processed text to surprise the user!</li>
        <li><b>Perplexity:</b> <a
                href="https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3">Perplexity</a> is
            a natural language processing concept that measures "how well a probability model predicts a sample." We
            incorporated this insight from our Markov model (see more below) to control the synthesis type and whether
            or not an LFO is enabled. Our design choice uses AM and FM synthesis as well as LFOs to make the sound
            progressively more chaotic sounding as the perplexity value increases. </li>
    </ul>
    <p>Trial and error: letter diversity metric (which is self created).</p>
    <p>Choosing sample text options: </p>

    <h3>
        Automated Composition
    </h3>

    <p>
        Our project uses a Markov model trained on Twinkle Twinkle Little Star to generate a tune derived from the notes
        we generated during the text processing. This Markov chain calculates the probability that a given note will
        be followed by every other note then randomly generates future notes, weighted by their probability.
    </p>

    <h3>
        Visualization
    </h3>

    <p>
        Our visualization takes in the finalized series of notes from the text processing and automated composition and
        generates a visual representation of the note sequence. A random set of CSS colors are selected, and each unique
        note pitch is mapped to each of the selected colors. Then, the visualizer progressively creates the pattern,
        taking into account the note pitch for the color and the note length for the amount of color added. Once we
        learned how to create a canvas with the radial pattern, we began trying a variety of different visualization
        patterns and settled on radial, diagonal, horizontal, and vertical.
    </p>

    <h3>
        Future Improvements
    </h3>

    <p>
        In the future, we hope to incorporate a feature for users to input a MIDI file to train the Markov chain, but we
        ran into many browser issues when trying to import the necessary functions. We also hope to add more NLP
        features from built in libraries, but ran into many issues with those as well.
    </p>

    <h3>Miscellaneous</h3>

    <p>
        We had initially designed the program to allow users to control various parameters for automated composition and
        digital synthesis, but throughout the project, we shifted our focus toward controlling these features with other
        forms of analysis of the user's writing. This project is intended to
    </p>

    <h3>Try It Out!</h3>

    <p>
        Select from three sample texts or input your own, and the program will generate a note series and visualization
        for the text. You can choose from four different types of visualization patterns: radial, diagonal, horizontal,
        and vertical.
    </p>

</body>

<script src="main.js"> </script>

</html>